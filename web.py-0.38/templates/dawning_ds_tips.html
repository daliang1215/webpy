<!DOCTYPE html>
<html>
<meta charset="UTF-8">
<title>曙光存储产品事业部 | Jimmy Chen | jimmy.f.chen@gmail.com</title>

<xmp theme="united" style="display:none;">
# ParaStor云存储系统内部Tips

修订前版本| 修订内容|完成日期| 修订人|修订后版本 
---|---|---|---|---
NULL|1.	软件版本说明<br> 2、OpenStack支持情况  |2017-11-04| 石静|V1.0
V1.0 |1.	修复性能、小文件聚合修复性能<br>2、oStor-G20-586配置说明<br> 3、白包装申请流程 | 2017-11-24| 石静|V1.1
V1.1 |1.	增加12Gb SAS硬盘与SATA硬盘混插的相关限制<br>2. 增加人工智能IO模型简介<br> | 2017-12-07| 石静|V1.2
V1.2 |1.	增加生物基因对存储的基本要求| 2018-1-05| 石静|V1.3
V1.3 |1.	增加OStor-G20-586适用场景<br> 2.	增加HPC行业配置注意事项| 2018-6-01| 石静|V1.4

## 产品功能
### ParaStor各软件版本说明
#### V2.1.X
+ 从此版本开始集成NAS功能，包括NFS、CIFS，FTP；
+ Windows POSIX客户端支持Windows Server 2008、Windows 7、Windows XP等；
+ Linux POSIX客户端支持RHEL、SUSE系列，内核要求是2.6以上；
+ 支持HDFS接口。

#### V2.5.0
版本功能包括如下：

+ 1)	RDMA：支持Infiniband、Intel 100Gb OPA网络；25Gb RDMA协议（RoCE）在科大讯飞项目中测试过，功能可以调通，但性能有待调优；不建议实际使用，暂时只做控标点；
+ 2)	集群NAS分区
+ 3)	小文件聚合；经实际测试，开启小文件聚合后，数据修复速度是之前的11倍；（系统80%是KB级小文件）；
+ 4)	QoS：针对广媒应用，通过智能数据预取保证平滑性能；
+ 5)	DNS负载均衡。

从V2.5.0版本开始，停止对Windows私有客户端的支持。

#### V2.5.1
版本说明包括如下：

+ 1)	DAC，增强的权限控制，实现CIFS与NFS权限统一管理；
+ 2)	SSD读缓存加速，支持SATA/SAS SSD，PCI-E SSD；
+ 3)	对象接口，Swift、S3；
+ 4)	回收站、远程复制、数据归档、分区节能仅作为控标项，研发系统测试未通过，暂时不再开发；
+ 5)	之前声称的NVDIMM尚未开发，V2.X版本不再支持。

### OpenStack支持情况

+ 1)	OpenStack主要跟存储对接的模块有三个 nova glance 和 cinder。nova负责虚拟机的启动盘也就是C盘和vda（sda）。目前主要的使用方式是，把各个虚拟机主机的instance目录挂载到统一存储上；此目录可以挂载ParaStor文件系统。
+ 2)	glace负责存储镜像，也就是启动盘的镜像，后端有目录和Swift两种主要方案，也就是glace只是一个转发代理；此模块无论是NAS还是Swift接口，理论上都可以与ParaStor对接。
+ 3)	cinder模块是负责块的，也就是给虚拟机提供D盘sdb用的。目前用法主要是使用各家的驱动对接盘阵，基本用法是用lvm。cinder块设备的快照功能等高级功能，主要是通过各家的驱动让后端设备支持。cinder模块有nfs的驱动，研发内网测试环境的驱动是用nfs驱动修改的。只能基本使用但不支持任何高级功能，比如基本的快照。


## 产品性能
### 修复性能
* 我们宣称的1TB/h的修复性能，前提是保证磁盘分组中的硬盘数量至少为96。
* POC测试时，一般是仅划分一个分组，并且前端无压力。
* 在实际应用中，磁盘分组中的硬盘数量一般为24~36，在无任何压力的时候，1TB数据的修复速度大约3小时。
* 经过测试发现，若业务类型是IOPS密集型，对修复性能影响不大，若业务类型是大块顺序读写带宽密集型，对修复影响很大。前端业务性能达到存储极限性能的50%时，将会导致系统修复速度极低。

### 小文件聚合修复性能
#### 测试环境
环境信息：2mos+4ds，标准v3环境，每节点24块4T磁盘，部署4+2：1文件系统，其它参数默认；<br>
预埋文件空间占比：大文件20%，小文件80%，共115T数据，接近系统总容量50%。<br>
预埋文件目录结构：D表示目录深度，W表示目录宽度，F表示每个目录中的文件数<br>

+ 256K：D=4,W=10,F=40000，共4亿个
+ 100M：D=2,F=10,F=640，共64000个
+ 300M：D=2,F=10,F=640，共64000个

#### 测试结果：

小文件聚合开关 | 修复性能（对象数/秒）| 修复时间
---|---|---
关闭| 73| 9天
开启 | 1-3 | 19小时

结论：

+ 1)	因小文件聚合开启\关闭会极大的改变对象大小，故性能对比要以系统显示修复时间来衡量；
+ 2)	开启小文件聚合后，修复性能提升约11倍（9×24/19=11.4）；
+ 3)	由于预埋文件中有20%为大文件，根据换算可得开启小文件聚合后，小文件修复性能提升值大致为14倍。


## 配置相关
### oStor-G20-586配置说明

+ 1)	必须配置2颗CPU，不能减配为1颗；
+ 2)	内存槽位为16；
+ 3)	PCI-E插槽仅有2个可用（供电）。
### oStor-G20-586适用场景

+ 1)	oStor-G20-586 节点仅仅建议在视频监控中推广；
+ 2)	此机型对应的是Grantly平台，E5-2620V4，不会同步升级的Purely平台；


### 白包装申请流程

某些情况下，销售会有白牌机的要求，目前无需额外工作，可以实现的白包装要求如下：

+ 1)	机器外包装不显示Sugon Logo、机器上综合标签不显示Sugon Logo ；
+ 2)	综合标签的产品型号仍显示OPara-G20、OStor-G20 ；
+ 3)	BIOS方面要求非曙光logo（服务器开机时的显示）；
+ 4)	ParaStor软件界面不做任何处理；若ParaStor界面不体现Sugon或产品相关信息，需要使用OEM版本，再单独讨论。

申请流程如下：

+ 1)	在OA系统->产品管理->特配申请中，分别针对OPara、OStor提交特配申请，生成新的BOM号。产品经理选择杨絮或乔雅楠。
其中，特配申请单中【包装】选项选择【中性包装（无Logo）】，如下所示：
![image](http://efeichn.f3322.net:1234/static/markdown/white_package.png)


+ 2)	特配号做好后，下单时使用OPara、OStor特配号及61005008（ukey，数量为1）下单。
+ 3)	下单后，同步在OA系统->产品生产->生产特殊操作申请中，提交该订单对应的《销售订单生产特殊操作申请》。具体请联系杨絮或乔雅楠。

### 12Gb SAS硬盘与SATA硬盘混插的相关限制
目前7.2krpm NL-SAS硬盘及10krpm SAS硬盘的接口速率为12Gb/s，7.2krpm SATA硬盘及ParaStor所用的SATA SSD接口速率为6Gb/s。之前OStor节点的变式BOM中是允许12Gb/s SAS 硬盘与SATA硬盘混插，只要总数≥12，且每种硬盘数量为2的倍数即可。<br>
近期在产线老化阶段，通过FIO测试工具全力压测磁盘的极限读性能时，发现SATA硬盘读带宽仅有几兆，通过几天排查验证，目前确认后置12块SATA性能低是由于前置的24块12Gb SAS硬盘抢占读性能导致，写性能比较均衡。 <br>
通过以下几种方式测试均无改善：<br>

+ 1)	将SAS卡两个口全部连接到前置背板，后置背板与前置背板串联；
+ 2)	将expander上行链路调整为6Gb或者12Gb；
+ 3)	将SATA硬盘和SAS硬盘无规则混插；
+ 4)	关闭部分前置背板连接的SAS上行PHY；
+ 5)	升级到Avago官方最新FW；
+ 6)  更新驱动；

> 通过与LSI沟通，此为SAS协议已知问题，无法解决。
此问题在4U24、4U36的Romley平台（V3/V4）、Purley平台均会出现，经过测试发现，12Gb SAS硬盘数量在9块时出现此问题，需要禁止混插。目前BOM中做了限定，混插情况下，12Gb SAS硬盘数量不得超过8块。<br>
但实际上ParaStor软件并不会发挥硬盘的极限读性能，同一套设备部署ParaStor后进行压力测试，并未发现此问题。BOM暂时先做此限定，后续如果混插情况很多的话，再去掉限制。<br>

### HPC 行业配置注意事项
+ 1)	HPC采用POSIX访问方式，Infiniband或OPA使用RDMA协议时，若前端计算节点客户端数量大于等于500时，索引节点、数据节点的内存需要增配到128GB或以上


## 行业应用
### 4.1	人工智能IO模型简介
深度学习的典型业务流程：重复利用大量数据训练集，多次计算，多次迭代。<br>
对存储系统的基本要求如下：<br>

+ 1)	共享：多台计算节点访问同样的训练集；统一存储
+ 2)	性能：实时满足各计算节点的访问请求；不同AI场景，不同训练模型，IO读写模式不同，要求的存储系统性能不同。以机器视觉（人脸识别、图像识别）、语音识别、文本识别为例，常见的IO读写模式如下：

+	  a)	训练集：百万甚至千万级KB级小文件，计算集群节点读取，计算完成后偶而写回极少数小文件；重点是KB级小文件的读IOPS。
+	    b)	训练集：百万级MB级文件，计算集群节点读取，计算完成后偶而写回极少数小文件；重点是MB级文件的读IOPS。
+	c)	训练集：单个GB级大文件，计算集群节点读取后切成MB级文件进行计算；重点是大文件单流带宽。
+	d)	训练集：单个大文件，计算集群节点随机读取文件的部分内容；重点是大文件跳读带宽

+ 3)	扩展性：动态扩容。
+ 4)	可靠性：持续计算的前提。
### 4.2	生物基因对存储的基本要求
+ 1)	生物基因小文件居多，海量小文件的管理和访问对存储系统提出很高要求；
+ 2)	前端客户端数量多，系统压力大，对存储的聚合性能要求较高，否则存储容易成为瓶颈；
+ 3)	多台客户端多次且频繁地打开、关闭文件，对元数据性能要求高，对系统OPS要求较高；
+ 4)	存储容量巨大，数据增长迅速，要求支持在线扩容；
+ 5)	数据有分层存储需求，近期（一般为一个月）访问的数据需要自动迁移到热数据区；
+ 6)	对Hadoop大数据处理有一定需求；
+ 7)	生物云、大数据是未来的发展方向。


> **updated by Jimmy Chen**

> **Date: 2018-6-01 儿童节**

</xmp>
<script src="/static/v/0.2/strapdown.js"></script>
</html>

